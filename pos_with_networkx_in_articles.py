# -*- coding: utf-8 -*-
"""POS with NetworkX in articles_Cyshild  .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18aAvR_KLMjJG34tk1DGyezB4AZTUL9E7

#POS with NetworkX in articles 
#Introduction:
The code demonstrates how to use natural language processing (NLP) tools to perform Part-Of-Speech (POS) tagging on an Arabic text dataset. The script uses several NLP libraries such as NLTK, FarasaSegmenter, and NetworkX to tokenize the text, remove stopwords, tag the text with POS tags, and visualize the resulting network graph.
 

#Data description:
The code reads an Arabic  articles dataset from a file named "arabic_dataset.txt" and preprocesses it using tokenization, stopword removal, and POS tagging. The resulting dataset is a list of POS tags for each word in the input text.


#Baseline experiments:
The goal of the baseline experiment is to perform POS tagging on the input text and visualize the resulting network graph. The script achieves this goal by using FarasaSegmenter to tag the text with POS tags and NetworkX to create a network graph of the resulting POS tags.

 
#Tools and external resources:
###The code uses several Python libraries for NLP tasks such as:

* NLTK for tokenization and stopword removal
* FarasaSegmenter for POS tagging
* NetworkX for creating a network graph of the POS tags
* matplotlib for visualizing the network graph
* The code also uses an external dataset named "arabic_dataset.txt" 
* that contains the Arabic text to be processed.

---



---



---



---

##Install the important library
"""

!pip install pyarabic
!pip  install -U farasapy

"""##Import the required library"""

import pandas as pd
import nltk
import networkx as nx
import matplotlib.pyplot as plt
from nltk.corpus import stopwords
from pyarabic.araby import strip_tashkeel, tokenize
from farasa.segmenter import FarasaSegmenter

"""## Download required resources for Arabic language processing

"""

nltk.download('stopwords')
nltk.download('punkt')

"""## Load Arabic text dataset

"""

with open('/content/arabic_dataset.txt', 'r', encoding='utf-8') as f:
    text = f.read()

"""##Body of the code """

# Pre-processing: tokenization, stopword removal, and POS tagging
tokens = tokenize(strip_tashkeel(text))
stop_words = set(stopwords.words('arabic'))
filtered_tokens = [w for w in tokens if not w in stop_words]


# Create a dictionary of word frequency counts
word_counts = {}
for token in filtered_tokens:
    if token in word_counts:
        word_counts[token] += 1
    else:
        word_counts[token] = 1

# Convert the list of tokens to a string
text_str = ' '.join(filtered_tokens)
print(text_str) 
 
# Create a FarasaPOSTagger instance and tag the segmented text
tagger = FarasaPOSTagger()
pos_tags = tagger.tag(text_str)


# Convert the string of POS tags back to a list
pos_tags = pos_tags.split()
print(pos_tags)

my_string = ', '.join(pos_tags)
print(my_string)

# Split the string on ","
split_txt = my_string.split(",")

# Split each element in the list on "/"
split_txt = [elem.split("/") for elem in split_txt]

# Print the resulting list
print(split_txt)
# Create a DataFrame from the pos_tags list
df = pd.DataFrame(split_txt, columns=["Token", "POS"])

# Print the DataFrame
df

# Create a network graph of POS tags
G = nx.DiGraph()

# Add nodes for each unique POS tag
unique_pos = set(pos_tags)
for pos in unique_pos:
    G.add_node(pos)

# Add edges between adjacent POS tags
for i in range(len(pos_tags)-1):
    G.add_edge(pos_tags[i], pos_tags[i+1])

# Visualize the network graph
pos = nx.spring_layout(G, seed=42)
nx.draw_networkx(G, pos, with_labels=True)

# maximize the size of the plot
fig = plt.gcf()
fig.set_size_inches(15, 15)
plt.show()

"""#Conclusion:
The code snippet demonstrates how to use NLP tools to perform POS tagging on Arabic text and visualize the resulting network graph. The resulting graph can be useful in analyzing the syntactic structure of the input text. The script can be extended to perform other NLP tasks such as named entity recognition and sentiment analysis.

#Answers to questions:

The biggest challenge in this project might be dealing with the complexity of the Arabic language and the lack of readily available datasets and tools for Arabic NLP tasks.
From this project, one can learn how to use various Python libraries for NLP tasks such as tokenization, stopword removal, and POS tagging. The project also demonstrates how to visualize the resulting data using NetworkX and matplotlib.

##END
"""